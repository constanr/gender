{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'tweets.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2b9f00096f56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# open the json file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tweets.json'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;31m# read the names and description fields from each tweet.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'tweets.json'"
     ]
    }
   ],
   "source": [
    "# In this example, I walk through how to use sklearn to classify users into male or female\n",
    "# based on their user description.\n",
    "\n",
    "# First, we need to get some tweets in JSON format.\n",
    "# Create a tweets.json file with something like:\n",
    "# twitter-curl --query \"track=obama\" > tweets.json\n",
    "# This will query twitter for tweets containing the word obama.\n",
    "\n",
    "# Now, we'll parse that file into a list of (name, description) tuples.\n",
    "import json\n",
    "import io\n",
    "# open the json file\n",
    "fp = io.open('tweets.json', mode='rt', encoding='utf8')\n",
    "# read the names and description fields from each tweet.\n",
    "data = []\n",
    "for line in fp:\n",
    "    js = json.loads(line)  # parse into a JSON object.\n",
    "    name = js['user']['name']\n",
    "    description = js['user']['description']\n",
    "    if name and description:  # if fields aren't blank\n",
    "        data.append((name.lower(), description.lower()))\n",
    "print 'read', len(data), 'users'\n",
    "print 'example:', data[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first male is james\n",
      "first female is mary\n",
      "ambiguous is jean\n",
      "got 473 males and 473 females\n"
     ]
    }
   ],
   "source": [
    "# Now, we need to label them as male or female. \n",
    "# To do that, we get the top 500 male/female names from census\n",
    "import requests  # This is a very handy library for html requests.\n",
    "males = requests.get('http://www.census.gov/genealogy/www/data/1990surnames/dist.male.first').text.split('\\n')\n",
    "males = [m.split()[0].lower() for m in males[:500]]  # lower case and take top 500\n",
    "print 'first male is', males[0]\n",
    "females = requests.get('http://www.census.gov/genealogy/www/data/1990surnames/dist.female.first').text.split('\\n')\n",
    "females = [f.split()[0].lower() for f in females[:500]]  # lower case and take top 500\n",
    "print 'first female is', females[0]\n",
    "\n",
    "# Remove ambiguous names (those that appear on both lists)\n",
    "# Note that the plus operator is overloaded to mean concatentation for lists.\n",
    "ambiguous = [f for f in females + males if f in males and f in females]\n",
    "print 'ambiguous is', ambiguous[0]\n",
    "males = [m for m in males if m not in ambiguous]\n",
    "females = [f for f in females if f not in ambiguous]\n",
    "print 'got', len(males), 'males and', len(females), 'females'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1637 males\n",
      "(u'daniel john sobieski', u\"editorial writer, investor's business daily, #reagan #conservative, somewhere to the right of attila the hun #catholic #prolife #tcot #teaparty #nra\")\n",
      "780 females\n",
      "(u'jessica cranor ', u'lame, nerdy, brilliant, fantastic')\n"
     ]
    }
   ],
   "source": [
    "# sort male, female users\n",
    "male_users = [d for d in data if len(d[0].split()) > 0 and d[0].split()[0] in males]\n",
    "print len(male_users), 'males'\n",
    "print male_users[0]\n",
    "female_users = [d for d in data if len(d[0].split()) > 0 and d[0].split()[0] in females]\n",
    "print len(female_users), 'females'\n",
    "print female_users[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first label= 0.0\n",
      "first description= editorial writer, investor's business daily, #reagan #conservative, somewhere to the right of attila the hun #catholic #prolife #tcot #teaparty #nra\n"
     ]
    }
   ],
   "source": [
    "# Make target vector. Female=1, Male=0\n",
    "import numpy as np\n",
    "y = np.array([0.] * len(male_users) + [1.] * len(female_users))\n",
    "data = [d[1] for d in male_users + female_users]\n",
    "print 'first label=', y[0]\n",
    "print 'first description=', data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "editorial writer, investor's business daily, #reagan #conservative, somewhere to the right of attila the hun #catholic #prolife #tcot #teaparty #nra \n",
      "is transformed into the sparse vector\n",
      "  (0, 662)\t1\n",
      "  (0, 1142)\t1\n",
      "  (0, 1274)\t1\n",
      "  (0, 1633)\t1\n",
      "  (0, 1846)\t1\n",
      "  (0, 2248)\t1\n",
      "  (0, 3396)\t1\n",
      "  (0, 3625)\t1\n",
      "  (0, 4803)\t1\n",
      "  (0, 4880)\t1\n",
      "  (0, 5480)\t1\n",
      "  (0, 5663)\t1\n",
      "  (0, 5863)\t1\n",
      "  (0, 6330)\t1\n",
      "  (0, 6663)\t1\n",
      "  (0, 6693)\t1\n",
      "  (0, 6757)\t2\n",
      "  (0, 6851)\t1\n",
      "  (0, 7522)\t1\n",
      "the word THE is mapped to index 6757\n",
      "there are 7662 unique features\n"
     ]
    }
   ],
   "source": [
    "# Convert descriptions into feature vectors.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(data)\n",
    "print data[0],'\\nis transformed into the sparse vector\\n', X[0]\n",
    "print 'the word THE is mapped to index', vec.vocabulary_['the']\n",
    "print 'there are', len(vec.vocabulary_), 'unique features'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg accuracy=0.807\n"
     ]
    }
   ],
   "source": [
    "# Compute cross validation accuracy\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "print 'avg accuracy=%.3f' % np.average(cross_validation.cross_val_score(clf, X, y, cv=5, scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg accuracy=0.752\n"
     ]
    }
   ],
   "source": [
    "# Try Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "print 'avg accuracy=%.3f' % np.average(cross_validation.cross_val_score(clf, X, y, cv=5, scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 27322 unique features\n",
      "ten feature examples: [u'16 33', u'my hobbies', u'loving faithful', u'member aft', u'twitter thing', u'whizzy_walexzy', u'rush daily', u'lom', u'se it', u'expect the']\n",
      "avg accuracy with bigrams=0.815\n"
     ]
    }
   ],
   "source": [
    "# Try adding bigrams\n",
    "vec = CountVectorizer(ngram_range=(1,2))\n",
    "X = vec.fit_transform(data)\n",
    "print 'there are', len(vec.vocabulary_), 'unique features'\n",
    "print 'ten feature examples:', vec.vocabulary_.keys()[:10]\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "print 'avg accuracy with bigrams=%.3f' % np.average(cross_validation.cross_val_score(clf, X, y, cv=5, scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "female words:\n",
      "mom=2.050\n",
      "mother=1.767\n",
      "christian=1.034\n",
      "wife=0.976\n",
      "grandmother=0.962\n",
      "girl=0.959\n",
      "lover=0.863\n",
      "fighter=0.848\n",
      "kind=0.736\n",
      "loving=0.722\n",
      "guns and=0.710\n",
      "one=0.697\n",
      "children=0.695\n",
      "lover of=0.693\n",
      "woman=0.671\n",
      "loves=0.659\n",
      "just don=0.643\n",
      "communications=0.643\n",
      "indian=0.642\n",
      "ayeeee=0.642\n",
      "\n",
      "\n",
      "male words:\n",
      "father=-1.423\n",
      "sports=-1.182\n",
      "husband=-1.146\n",
      "of the=-0.873\n",
      "veteran=-0.869\n",
      "love jesus=-0.806\n",
      "radio=-0.726\n",
      "dad=-0.704\n",
      "man=-0.703\n",
      "boy=-0.691\n",
      "jesus wife=-0.687\n",
      "consurvative love=-0.687\n",
      "consurvative=-0.687\n",
      "army=-0.644\n",
      "editor=-0.641\n",
      "guy=-0.639\n",
      "liberty=-0.638\n",
      "my wife=-0.614\n",
      "your=-0.606\n",
      "entrepreneur=-0.585\n"
     ]
    }
   ],
   "source": [
    "# Print top feature weights for female\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X, y)  # fit on all data\n",
    "top_indices = clf.coef_[0].argsort()[::-1] # sort in decreasing order\n",
    "# reverse the alphabet to map from idx->word\n",
    "vocab_r = dict((idx, word) for word, idx in vec.vocabulary_.iteritems())\n",
    "print 'female words:\\n', '\\n'.join(['%s=%.3f' % (vocab_r[idx], clf.coef_[0][idx]) for idx in top_indices[:20]])\n",
    "top_indices = clf.coef_[0].argsort() # sort in increasing order\n",
    "print '\\n\\nmale words:\\n', '\\n'.join(['%s=%.3f' % (vocab_r[idx], clf.coef_[0][idx]) for idx in top_indices[:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first document with reduced representation\n",
      "[ 3.31207804 -3.67943496]\n",
      "first PCA dimension (eigenvector): [ 0.00066414  0.00066414 -0.00033876 ...,  0.00021401  0.00021401\n",
      "  0.00021401]\n",
      "top words of first dimension:\n",
      "the=0.683\n",
      "of=0.443\n",
      "and=0.244\n",
      "to=0.217\n",
      "in=0.139\n",
      "of the=0.109\n",
      "is=0.082\n",
      "to the=0.075\n",
      "on=0.063\n",
      "nra=0.062\n",
      "right=0.061\n",
      "conservative=0.060\n",
      "the right=0.060\n",
      "editorial=0.058\n",
      "teaparty=0.057\n",
      "daily=0.057\n",
      "tcot teaparty=0.057\n",
      "prolife tcot=0.056\n",
      "right of=0.056\n",
      "prolife=0.056\n",
      "second PCA dimension (eigenvector): [-0.00038728 -0.00038728 -0.00027458 ...,  0.00030366  0.00030366\n",
      "  0.00030366]\n",
      "top words of second dimension:\n",
      "and=0.729\n",
      "in=0.166\n",
      "my=0.147\n",
      "love=0.090\n",
      "is=0.074\n",
      "country=0.065\n",
      "for=0.063\n",
      "it=0.056\n",
      "am=0.055\n",
      "wife=0.050\n",
      "mother=0.043\n",
      "at=0.041\n",
      "that=0.039\n",
      "politics=0.039\n",
      "god=0.039\n",
      "as=0.036\n",
      "all=0.033\n",
      "with=0.031\n",
      "good=0.031\n",
      "we=0.030\n",
      "avg accuracy using only 2 dimensions=0.680\n"
     ]
    }
   ],
   "source": [
    "# Use PCA to reduce the dimensionality of X to only 2 dimensions,\n",
    "# then compute cross-validation accuracy of resulting data X2.\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "X2 = pca.fit_transform(X.toarray())\n",
    "print 'first document with reduced representation'\n",
    "print X2[0]\n",
    "dim1 = pca.components_[0]\n",
    "print 'first PCA dimension (eigenvector):', dim1\n",
    "top_indices = dim1.argsort()[::-1]\n",
    "print 'top words of first dimension:\\n', '\\n'.join(['%s=%.3f' % (vocab_r[idx], dim1[idx]) for idx in top_indices[:20]])\n",
    "dim2 = pca.components_[1]\n",
    "print 'second PCA dimension (eigenvector):', dim2\n",
    "top_indices = dim2.argsort()[::-1]\n",
    "print 'top words of second dimension:\\n', '\\n'.join(['%s=%.3f' % (vocab_r[idx], dim2[idx]) for idx in top_indices[:20]])\n",
    "print 'avg accuracy using only 2 dimensions=%.3f' % np.average(cross_validation.cross_val_score(clf, X2, y, cv=5, scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
